{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 6\n"
     ]
    }
   ],
   "source": [
    "import setGPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yRDenD5M0rDY"
   },
   "outputs": [],
   "source": [
    "from typing import Any, Generator, Tuple, Mapping, Sequence, Optional, Callable\n",
    "from collections import namedtuple\n",
    "import functools, inspect, time, random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from absl import app\n",
    "import haiku as hk\n",
    "import jax\n",
    "from jax.experimental import optix\n",
    "from jax.tree_util import tree_multimap, tree_map, tree_reduce\n",
    "import jax.numpy as jnp\n",
    "from jax.lax import fori_loop\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_federated as tff\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S-RWB9G48uJA"
   },
   "outputs": [],
   "source": [
    "Batch = Mapping[str, jnp.ndarray]\n",
    "DatasetSequence = Sequence[tf.data.Dataset]\n",
    "LossFunction = Callable[[hk.Params, Batch], jnp.ndarray]\n",
    "OptState = Any\n",
    "ClientState = namedtuple(\"ClientState\", \"next_idx\\\n",
    "                                         epoch_count\")\n",
    "\n",
    "\n",
    "# define hyperparameters format.\n",
    "ServerHyperParams = namedtuple(\"ServerHyperParams\", \"num_rounds\\\n",
    "                                                     max_epochs\\\n",
    "                                                     num_batches\\\n",
    "                                                     batch_size\\\n",
    "                                                     seed\")\n",
    "\n",
    "# message to the client from server.\n",
    "ClientMessage = namedtuple(\"ClientMessage\", \"params\\\n",
    "                                             opt_init_input\")\n",
    "\n",
    "# message to the server from client.\n",
    "ServerMessage = namedtuple(\"ServerMessage\", \"aggregator_input\\\n",
    "                                             stateupdater_input\")\n",
    "\n",
    "\n",
    "# message to the server from client for book keeping.\n",
    "DiagnosticsMessage = namedtuple(\"DiagnosticsMessage\", \"train_loss\\\n",
    "                                                       train_acc\\\n",
    "                                                       test_loss\\\n",
    "                                                       test_acc\\\n",
    "                                                       weight\")\n",
    "ClientOutput = Tuple[ServerMessage, DiagnosticsMessage]\n",
    "\n",
    "# extracts messages from a list of client outputs.\n",
    "@jax.partial(jax.jit, static_argnums=[1])  # fix extractor.\n",
    "def extract_from_cout(\n",
    "    couts: Sequence[ClientOutput],\n",
    "    extractor: Callable[[ClientOutput], Any]\n",
    "    ) -> Sequence[Any]:\n",
    "  print(\"compiling: {}\".format(inspect.currentframe().f_code.co_name))\n",
    "  # TODO: use pytrees.transpose?\n",
    "  # msgs = fori_loop(0, len(couts),\n",
    "  #                    lambda i, msgs: msgs.append(extractor(couts[i])), [])\n",
    "  msgs = [extractor(cout) for cout in couts]\n",
    "  return msgs\n",
    "\n",
    "def make_jax_batch(batch: Mapping[str, tf.Tensor]) -> Batch:\n",
    "    \"\"\"Transform a Mapping[str, tf.Tensor] (tensorflow batch)\n",
    "    into a Mapping[str, jnp.array] (jax-compatible batch).\"\"\"\n",
    "    jax_batch = {}\n",
    "    for key in batch:\n",
    "        jax_batch[key] = jnp.array(batch[key].numpy())\n",
    "    return jax_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_2017annie_predict_EVD():\n",
    "    X = np.loadtxt('../data/private/predict_EVD/X.csv', skiprows=1, delimiter=',')\n",
    "    y = np.loadtxt('../data/private/predict_EVD/y.csv', skiprows=1, delimiter=',')\n",
    "    assert (X.shape[0] == y.shape[0])\n",
    "    return X,y\n",
    "\n",
    "# This function is not stateless, due to the use of random.shuffle. It is used only once, before the training loop.\n",
    "def split_by_age(\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        hyperparams: ServerHyperParams,\n",
    "        requesting_client: int = 0,\n",
    "        test_ratio: float = 0.5\n",
    ") -> Tuple[DatasetSequence, tf.data.Dataset]:\n",
    "    \"\"\" Split the datum points (X,y) into three datasets based on the age of the patients.\n",
    "    The patients are sorted into age groups [1-20y, 21-40y, >40y].\n",
    "    The three sets are shuffled.\n",
    "    \n",
    "    requesting_client: Index of the age group for which a test set is created.\n",
    "    \"\"\"\n",
    "    \n",
    "    def make_ds(idx):\n",
    "        return tf.data.Dataset.from_tensor_slices({'features': X[idx], 'label':y[idx]})\\\n",
    "            .batch(hyperparams.batch_size)\n",
    "    \n",
    "    ages = pd.read_csv('../data/private/0_raw_EbolaDatabase.csv')['age'].to_numpy()\n",
    "    age_lims = [-1, 20, 40, 1e3]\n",
    "    dss, ds_test = [0], None\n",
    "    \n",
    "    for i, (lower, upper) in enumerate(zip(age_lims[:-1], age_lims[1:])):\n",
    "        idx = np.asarray((lower < ages) & (ages <= upper)).nonzero()[0]\n",
    "        random.shuffle(idx)\n",
    "        if i == requesting_client:\n",
    "            test_size = hyperparams.batch_size * (1 + int(len(idx) // hyperparams.batch_size * test_ratio))\n",
    "            ds_test = make_ds(idx[:test_size])\n",
    "            dss[0] = make_ds(idx[test_size:])\n",
    "        else:\n",
    "            dss.append( make_ds(idx) )\n",
    "    \n",
    "    if ds_test is None:\n",
    "        raise ValueError(\"Unexpected 'requesting_client' argument: Expecting a non-negative int lower or equal to {}.\".format(len(age_lims)-2))\n",
    "    \n",
    "    return dss, ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0., ...,   0.,   1.,   0.],\n",
       "       [  1.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "       [  2.,   1.,   0., ...,   0.,   0.,   1.],\n",
       "       ...,\n",
       "       [572.,   0.,   0., ...,   0.,   0.,   1.],\n",
       "       [573.,   0.,   0., ...,   1.,   0.,   1.],\n",
       "       [574.,   0.,   0., ...,   0.,   0.,   1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = load_2017annie_predict_EVD()\n",
    "X_id = np.hstack((np.arange(X.shape[0]).reshape((X.shape[0], 1)), X))\n",
    "X_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dss, ds_test = split_by_age(X_id, y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(ds_test):\n",
    "    print(type(batch))\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GbTmc5DwjoON"
   },
   "outputs": [],
   "source": [
    "# Neural network model.\n",
    "def net_fn(batch: Batch) -> jnp.ndarray:\n",
    "  \"\"\"Logistic regression for 2-class classification.\"\"\"\n",
    "  x = batch[\"features\"]\n",
    "  logistic_reg = hk.Sequential([\n",
    "      hk.Flatten(),\n",
    "      hk.Linear(2), jax.nn.log_softmax\n",
    "  ])\n",
    "  return logistic_reg(x)\n",
    "net: hk.Transformed = hk.transform(net_fn)\n",
    "\n",
    "# Initialize neural network parameters \n",
    "def init(rng: jax.random.PRNGKey, batch: Batch) -> hk.Params:\n",
    "  return net.init(rng, batch)\n",
    "\n",
    "# get predictions from model.\n",
    "def forward(params: hk.Params, batch: Batch):\n",
    "  return jax.jit(net.apply)(params, batch)\n",
    "\n",
    "\n",
    "# Training loss (cross-entropy).\n",
    "@jax.jit\n",
    "def loss(params: hk.Params, batch: Batch) -> jnp.ndarray:\n",
    "  \"\"\"Compute the loss of the network, including L2.\"\"\"\n",
    "  print(\"compiling: {}\".format(inspect.currentframe().f_code.co_name))\n",
    "  preds = forward(params, batch)\n",
    "  labels = hk.one_hot(batch[\"label\"], 2)\n",
    "  # TODO: Put weight decay into optimizer\n",
    "  l2_loss = 0.5 * sum(jnp.sum(jnp.square(p)) for p in jax.tree_leaves(params))\n",
    "  softmax_xent = -jnp.mean(labels * preds)\n",
    "  return softmax_xent + 1e-4 * l2_loss\n",
    "\n",
    "# Evaluation metric (classification accuracy).\n",
    "@jax.jit\n",
    "def accuracy(params: hk.Params, batch: Batch) -> jnp.ndarray:\n",
    "  print(\"compiling: {}\".format(inspect.currentframe().f_code.co_name))\n",
    "  preds = forward(params, batch)\n",
    "  pred_class = jnp.argmax(preds, axis=-1)\n",
    "  return jnp.mean(pred_class == batch[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "elJQ6h8MVeH6"
   },
   "outputs": [],
   "source": [
    "# one local update step.\n",
    "@jax.partial(jax.jit, static_argnums=[2,4])  # fix loss function and optimizer.\n",
    "def run_one_step(\n",
    "    params: hk.Params,\n",
    "    batch: Batch,\n",
    "    client_opt: optix.InitUpdate,\n",
    "    opt_state: OptState,\n",
    "    loss: LossFunction\n",
    "    ) -> Tuple[hk.Params, OptState]:\n",
    "  \"\"\"Learning rule (stochastic gradient descent).\"\"\"\n",
    "  print(\"compiling: {}\".format(inspect.currentframe().f_code.co_name))\n",
    "  grads = jax.grad(loss)(params, batch)\n",
    "  updates, opt_state = client_opt.update(grads, opt_state)\n",
    "  new_params = optix.apply_updates(params, updates)\n",
    "  return new_params, opt_state  \n",
    "\n",
    "# perform client updates.\n",
    "def client_updater(\n",
    "    msg: ClientMessage,\n",
    "    ds: Sequence[Batch],\n",
    "    client_opt: optix.InitUpdate,\n",
    "    loss: LossFunction\n",
    "    ) -> ClientOutput:\n",
    "  opt_state = client_opt.init(*msg.opt_init_input)\n",
    "  # iterate through data making updates.\n",
    "  new_params = msg.params\n",
    "  for minibatch in ds:\n",
    "    new_params, opt_state = run_one_step(new_params,\n",
    "                                         minibatch,\n",
    "                                         client_opt, \n",
    "                                         opt_state, \n",
    "                                         loss)\n",
    "  # compute and return the change in parameters.\n",
    "  params_update = tree_multimap(lambda x, y: x - y, new_params, msg.params)\n",
    "  \n",
    "  # TODO: replace with an function which constructs message  \n",
    "  msg_to_server = ServerMessage(\n",
    "      aggregator_input=params_update,\n",
    "      stateupdater_input=None\n",
    "  )\n",
    "\n",
    "  diagnostic_msg = DiagnosticsMessage(\n",
    "      train_loss=0,\n",
    "      train_acc=1,\n",
    "      test_loss=0,\n",
    "      test_acc=1,\n",
    "      weight=1\n",
    "  )\n",
    "  return msg_to_server, diagnostic_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zvRbtDM1wS_s"
   },
   "outputs": [],
   "source": [
    "# aggregate client updates.\n",
    "# TODO: make the aggregator stateful.\n",
    "@jax.jit\n",
    "def average_params(params_list: Sequence[hk.Params]) -> hk.Params: \n",
    "  print(\"compiling: {}\".format(inspect.currentframe().f_code.co_name))\n",
    "  num_params = len(params_list)\n",
    "  params_sum = functools.reduce(\n",
    "      lambda t1, t2: tree_multimap(sum, t1, t2), params_list)\n",
    "  params_avg = tree_map(lambda x: x/num_params, params_sum)\n",
    "  return params_avg\n",
    "\n",
    "def calc_distances(updates_list: Sequence[hk.Params], relevant_client: int) -> Sequence[Any]:\n",
    "    return [tree_reduce(\n",
    "                sum,\n",
    "                tree_multimap(lambda x, y: (x-y)**2, updates, updates_list[relevant_client])\n",
    "                ) for updates in updates_list ]\n",
    "\n",
    "def similarity_aggregator(updates_list: Sequence[hk.Params], relevant_client: int):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolutely not suitable for jit-compilation.\n",
    "def make_client_data(\n",
    "    ds: tf.data.Dataset,\n",
    "    client_state: ClientState,\n",
    "    hyperparams: ServerHyperParams\n",
    ") -> Tuple[Sequence[Batch], ClientState]:\n",
    "  batches_before, batches_after = [], []\n",
    "  \n",
    "  # Initially skip all the way to client_state.next_idx:\n",
    "  for i, batch in enumerate(ds):\n",
    "    if i < client_state.next_idx:\n",
    "      batches_before.append(\n",
    "        make_jax_batch(batch)\n",
    "      )\n",
    "    # Return once num_batches batches have been selected:\n",
    "    elif len(batches_after) >= hyperparams.num_batches:\n",
    "      client_state = ClientState(next_idx = i,\n",
    "                                 epoch_count = client_state.epoch_count)\n",
    "      return batches_after, client_state\n",
    "    # Transform the tf batch to a Batch (Mapping[str, jnp.array])\n",
    "    else:\n",
    "      batches_after.append(\n",
    "        make_jax_batch(batch)\n",
    "      )\n",
    "  \n",
    "  # Traverse the dataset up to max_epochs-1 more times, until num_batches\n",
    "  # batches are in batches_after\n",
    "  for epoch in range(hyperparams.max_epochs - 1):\n",
    "    # Increment the client's epoch_count\n",
    "    client_state = ClientState(next_idx = 0,\n",
    "                               epoch_count = client_state.epoch_count + 1)\n",
    "    \n",
    "    for i, batch in enumerate(ds):\n",
    "      # Return once num_batches batches have been selected:\n",
    "      if len(batches_after) >= hyperparams.num_batches:\n",
    "        client_state = ClientState(next_idx = i,\n",
    "                                   epoch_count = client_state.epoch_count)\n",
    "        return batches_after, client_state\n",
    "      # Transform the tf batch to a Batch (Mapping[str, jnp.array])\n",
    "      batches_after.append(\n",
    "        make_jax_batch(batch)\n",
    "      )\n",
    "  \n",
    "  # If num_batches has not yet been reached,\n",
    "  # add (some or all of) the batches_before at the end:\n",
    "  num_to_add = hyperparams.num_batches - len(batches_after)\n",
    "  batches_to_add = batches_before[:num_to_add]\n",
    "  client_state = ClientState(next_idx = len(batches_to_add),\n",
    "                             epoch_count = client_state.epoch_count)\n",
    "  batches_after += batches_to_add\n",
    "  return batches_after, client_state\n",
    "\n",
    "def make_all_client_data(\n",
    "    client_data: Sequence[tf.data.Dataset],\n",
    "    client_states: Sequence[ClientState],\n",
    "    hyperparams: ServerHyperParams\n",
    ") -> Tuple[Sequence[Sequence[Batch]], Sequence[ClientState]]:\n",
    "  if not (len(client_states) == len(client_data)):\n",
    "    raise ValueError(\n",
    "        \"Number of datasets ({}) does not fit number of client \"\n",
    "        \"states ({})!\".format(len(client_data), len(client_states))\n",
    "        )\n",
    "  dss_jax, client_states_after = [], []\n",
    "  for ds, client_state_before in zip(client_data, client_states):\n",
    "    batches_jax, client_state_after = make_client_data(ds, client_state_before, hyperparams)\n",
    "    dss_jax.append(batches_jax)\n",
    "    client_states_after.append(client_state_after)\n",
    "  return dss_jax, client_states_after\n",
    "\n",
    "def init_client_states(client_data: Sequence[tf.data.Dataset]) -> Sequence[ClientState]:\n",
    "  return [ClientState(0,0) for i in range(len(client_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c9V22gJC2DIn"
   },
   "outputs": [],
   "source": [
    "# aggregate diagnositcs.\n",
    "@jax.jit\n",
    "def agg_diagnostics(\n",
    "    client_outputs: Sequence[ClientOutput]) -> DiagnosticsMessage:\n",
    "  d_msgs = extract_from_cout(client_outputs, lambda cout: cout[1])\n",
    "  # TODO: aggregate and report statistics\n",
    "  return d_msgs[0]\n",
    "\n",
    "\n",
    "# update server params.\n",
    "@jax.partial(jax.jit, static_argnums=[2,4])  # fix optimizer and aggregator.\n",
    "def server_updater(\n",
    "    server_params: hk.Params,\n",
    "    client_outputs: Sequence[ClientOutput],\n",
    "    server_opt: optix.InitUpdate,\n",
    "    opt_state: OptState,\n",
    "    aggregator: Callable[[Sequence[Any]], hk.Params]\n",
    "    ) -> Tuple[hk.Params, OptState]:\n",
    "  print(\"compiling: {}\".format(inspect.currentframe().f_code.co_name))\n",
    "  agg_inputs = extract_from_cout(client_outputs,\n",
    "                                 lambda cout: cout[0].aggregator_input)\n",
    "  agg_update = aggregator(agg_inputs)\n",
    "  eff_grads = tree_map(lambda x: -1.0*x, agg_update)  # effective gradient.\n",
    "  updates, opt_state = server_opt.update(eff_grads, opt_state)\n",
    "  # TODO: allow opt_state to also be explicitly updated by state_updater\n",
    "  server_params = optix.apply_updates(server_params, updates)\n",
    "  return server_params, opt_state\n",
    "\n",
    "# one round of federated learning.\n",
    "def run_one_round(\n",
    "    server_params: hk.Params,\n",
    "    hyperparams: ServerHyperParams,\n",
    "    client_data: Sequence[tf.data.Dataset],\n",
    "    client_states: Sequence[ClientState],\n",
    "    client_opt: optix.InitUpdate,\n",
    "    server_opt: optix.InitUpdate,\n",
    "    opt_state: OptState,\n",
    "    aggregator: Callable[[Sequence[Any]], hk.Params],\n",
    "    loss: LossFunction,\n",
    "    rng: jax.random.PRNGKey\n",
    ") -> Tuple[hk.Params, OptState]:\n",
    "  \n",
    "  print(\"computing updates from active clients.\")\n",
    "  # TODO: replace with an function which constructs message\n",
    "  msg_to_clients = ClientMessage(\n",
    "      params=server_params, \n",
    "      opt_init_input=[server_params])\n",
    "  \n",
    "  dss, client_states_out = make_all_client_data(client_data, client_states, hyperparams)\n",
    "  \n",
    "  client_outputs = list(map(lambda ds: client_updater(\n",
    "                          msg_to_clients,\n",
    "                          ds,\n",
    "                          client_opt,\n",
    "                          loss), dss))\n",
    "  \n",
    "  print(\"aggregating client updates.\")\n",
    "  server_params, opt_state = server_updater(\n",
    "      server_params,\n",
    "      client_outputs,      \n",
    "      server_opt,      \n",
    "      opt_state,      \n",
    "      aggregator)\n",
    "  \n",
    "  # TODO: aggregate and incporate new diagnostics. needs state!\n",
    "  diag_state = agg_diagnostics(client_outputs)\n",
    "  \n",
    "  return server_params, client_states_out, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "colab_type": "code",
    "id": "BuZcD4dEdzF3",
    "outputId": "08edffde-2229-4389-e8b7-bcd7c00320e2"
   },
   "outputs": [],
   "source": [
    "# Testing one round of federated averaging.\n",
    "batch_size=10\n",
    "dss, ds_test = split_by_age(*load_2017annie_predict_EVD(), batch_size)\n",
    "client_states = init_client_states(dss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "colab_type": "code",
    "id": "BuZcD4dEdzF3",
    "outputId": "08edffde-2229-4389-e8b7-bcd7c00320e2"
   },
   "outputs": [],
   "source": [
    "params = init(jax.random.PRNGKey(42), dss[0].as_numpy_iterator().next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "colab_type": "code",
    "id": "BuZcD4dEdzF3",
    "outputId": "08edffde-2229-4389-e8b7-bcd7c00320e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing updates from active clients.\n",
      "compiling: run_one_step\n",
      "compiling: loss\n",
      "compiling: run_one_step\n",
      "compiling: loss\n",
      "aggregating client updates.\n",
      "compiling: server_updater\n",
      "compiling: extract_from_cout\n",
      "compiling: average_params\n",
      "compiling: extract_from_cout\n"
     ]
    }
   ],
   "source": [
    "client_opt = optix.sgd(0.1)\n",
    "server_opt = optix.sgd(1.0)\n",
    "opt_state = server_opt.init(params)\n",
    "rng = jax.random.PRNGKey(0)\n",
    "hyperparams = ServerHyperParams(\n",
    "    num_rounds = 12,\n",
    "    max_epochs = 1,\n",
    "    num_batches = 5000,\n",
    "    seed = 7\n",
    ")\n",
    "\n",
    "run_one_round(\n",
    "    params,\n",
    "    hyperparams,\n",
    "    dss,\n",
    "    client_states,\n",
    "    client_opt,\n",
    "    server_opt,\n",
    "    opt_state,\n",
    "    average_params,\n",
    "    loss,\n",
    "    rng\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MsHof54fApTp"
   },
   "outputs": [],
   "source": [
    "def federated_learning(\n",
    "    hyperparams: ServerHyperParams,\n",
    "    client_data: Sequence[tf.data.Dataset],\n",
    "    client_opt: optix.InitUpdate,\n",
    "    server_opt: optix.InitUpdate,\n",
    "    aggregator: Callable[[Sequence[hk.Params]], hk.Params],\n",
    "    loss: LossFunction,\n",
    "    init\n",
    "    ) -> hk.Params:\n",
    "  # initialize random generator, params, opt_state.\n",
    "  rng = jax.random.PRNGKey(hyperparams.seed)\n",
    "  # TODO: use synthethic data for init\n",
    "  server_params = init(rng, client_data[0].as_numpy_iterator().next())\n",
    "  client_states = init_client_states(client_data)\n",
    "  opt_state = server_opt.init(server_params)\n",
    "  rngs = jax.random.split(rng, hyperparams.num_rounds)\n",
    "  for round_num, rng in enumerate(rngs):\n",
    "    print(\"\\nrunning round {}\".format(round_num))\n",
    "    server_params, \\\n",
    "    client_states, \\\n",
    "    opt_state       = run_one_round(server_params, hyperparams, \n",
    "                                    client_data, client_states, client_opt, \n",
    "                                    server_opt, opt_state, \n",
    "                                    aggregator, loss, rng)\n",
    "  return server_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_test_score(params: hk.Params, ds: tf.data.Dataset) -> jnp.ndarray:\n",
    "    accs = []\n",
    "    for batch in ds:\n",
    "        accs.append(accuracy(params, make_jax_batch(batch)))\n",
    "    return np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "colab_type": "code",
    "id": "iT-dXeovX2Fd",
    "outputId": "57c040e3-f579-497a-bb96-d30bed2d2004",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grimberg/.local/share/virtualenvs/semester-project-privateML-TGzbS9iS/lib/python3.7/site-packages/jax/lib/xla_bridge.py:123: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n",
      "/home/grimberg/.local/share/virtualenvs/semester-project-privateML-TGzbS9iS/lib/python3.7/site-packages/jax/lax/lax.py:5104: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  warnings.warn(msg.format(dtype, fun_name , truncated_dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "running round 0\n",
      "computing updates from active clients.\n",
      "compiling: run_one_step\n",
      "compiling: loss\n",
      "aggregating client updates.\n",
      "compiling: server_updater\n",
      "compiling: extract_from_cout\n",
      "compiling: average_params\n",
      "compiling: extract_from_cout\n",
      "\n",
      "running round 1\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 2\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 3\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 4\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 5\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 6\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 7\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 8\n",
      "computing updates from active clients.\n",
      "compiling: run_one_step\n",
      "compiling: loss\n",
      "aggregating client updates.\n",
      "\n",
      "running round 9\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 10\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 11\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 12\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 13\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 14\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 15\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 16\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 17\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 18\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 19\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 20\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 21\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 22\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 23\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 24\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 25\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 26\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 27\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 28\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 29\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "compiling: accuracy\n",
      "\n",
      "running round 0\n",
      "computing updates from active clients.\n",
      "compiling: run_one_step\n",
      "aggregating client updates.\n",
      "compiling: server_updater\n",
      "compiling: extract_from_cout\n",
      "compiling: average_params\n",
      "compiling: extract_from_cout\n",
      "\n",
      "running round 1\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 2\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 3\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 4\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 5\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 6\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 7\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 8\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 9\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 10\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 11\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 12\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 13\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 14\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 15\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 16\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 17\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 18\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 19\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 20\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 21\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 22\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 23\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 24\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 25\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 26\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 27\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 28\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 29\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 0\n",
      "computing updates from active clients.\n",
      "compiling: run_one_step\n",
      "aggregating client updates.\n",
      "compiling: server_updater\n",
      "compiling: extract_from_cout\n",
      "\n",
      "running round 1\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 2\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 3\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 4\n",
      "computing updates from active clients.\n",
      "compiling: run_one_step\n",
      "aggregating client updates.\n",
      "\n",
      "running round 5\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 6\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 7\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 8\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 9\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 10\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 11\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 12\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 13\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 14\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 15\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 16\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 17\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 18\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 19\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 20\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 21\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 22\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 23\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 24\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 25\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 26\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 27\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 28\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 29\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 0\n",
      "computing updates from active clients.\n",
      "compiling: run_one_step\n",
      "aggregating client updates.\n",
      "compiling: server_updater\n",
      "compiling: extract_from_cout\n",
      "\n",
      "running round 1\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 2\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 3\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 4\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 5\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 6\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 7\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 8\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 9\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 10\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 11\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 12\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 13\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 14\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 15\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 16\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 17\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 18\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 19\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 20\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 21\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 22\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 23\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 24\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 25\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 26\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 27\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 28\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 29\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 0\n",
      "computing updates from active clients.\n",
      "compiling: run_one_step\n",
      "aggregating client updates.\n",
      "compiling: server_updater\n",
      "compiling: extract_from_cout\n",
      "\n",
      "running round 1\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 2\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 3\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 4\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 5\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 6\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 7\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 8\n",
      "computing updates from active clients.\n",
      "compiling: run_one_step\n",
      "aggregating client updates.\n",
      "\n",
      "running round 9\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 10\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 11\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 12\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 13\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 14\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 15\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 16\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 17\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 18\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 19\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 20\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 21\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 22\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 23\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 24\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 25\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 26\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 27\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 28\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 29\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 0\n",
      "computing updates from active clients.\n",
      "compiling: run_one_step\n",
      "aggregating client updates.\n",
      "compiling: server_updater\n",
      "compiling: extract_from_cout\n",
      "\n",
      "running round 1\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 2\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 3\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 4\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 5\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 6\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 7\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 8\n",
      "computing updates from active clients.\n",
      "compiling: run_one_step\n",
      "aggregating client updates.\n",
      "\n",
      "running round 9\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 10\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 11\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 12\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 13\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 14\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 15\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 16\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 17\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 18\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 19\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 20\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 21\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 22\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 23\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 24\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 25\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 26\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 27\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 28\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n",
      "\n",
      "running round 29\n",
      "computing updates from active clients.\n",
      "aggregating client updates.\n"
     ]
    }
   ],
   "source": [
    "# testing federated learning code.\n",
    "\n",
    "hyperparams = ServerHyperParams(\n",
    "    num_rounds = 30,\n",
    "    max_epochs = 1,\n",
    "    num_batches = 3,\n",
    "    batch_size = 10,\n",
    "    seed = 420\n",
    ")\n",
    "\n",
    "random.seed(hyperparams.seed)\n",
    "\n",
    "test_scores = {'global': [], 'local': []}# , 'similar': []}\n",
    "for requesting_client in range(3):\n",
    "    dss, ds_test = split_by_age(*load_2017annie_predict_EVD(), hyperparams, requesting_client)\n",
    "\n",
    "    opt_params_global = federated_learning(\n",
    "        hyperparams,\n",
    "        dss,\n",
    "        optix.sgd(0.1),\n",
    "        optix.sgd(1.0),\n",
    "        average_params,\n",
    "        loss,\n",
    "        init\n",
    "        );\n",
    "    test_scores['global'].append( eval_test_score(opt_params_global, ds_test) )\n",
    "\n",
    "    opt_params_local = federated_learning(\n",
    "        hyperparams,\n",
    "        dss[requesting_client:requesting_client+1],\n",
    "        optix.sgd(0.1),\n",
    "        optix.sgd(1.0),\n",
    "        average_params,\n",
    "        loss,\n",
    "        init\n",
    "        );\n",
    "    test_scores['local'].append( eval_test_score(opt_params_local, ds_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3dfZQV1Znv8e9v8KU1EgRB4wBK60WjBhE9wvgSjUlU4oygcRwhk1lgEjEvGE3uMFcnWZHBSTRObhIzIQY0GpOrQjTGQePL4Au6jEq6GUEEQ0TU2IwraQFfGAMKPvePqgNFc7q7Wk6d03T/Pmud1VW7dtV5ujicp3ftqr0VEZiZmbX1F/UOwMzMuicnCDMzq8gJwszMKnKCMDOzipwgzMysol3qHUC1DBw4MIYNG1bvMMzMdiqLFi16NSIGVdrWYxLEsGHDaG5urncYZmY7FUkvtbfNl5jMzKwiJwgzM6vICcLMzCrqMX0Qlbzzzju0tLSwYcOGeodSVw0NDQwZMoRdd9213qGY2U6kRyeIlpYW+vbty7Bhw5BU73DqIiJYs2YNLS0tNDY21juc3icCsp+9tutm3ViPvsS0YcMG9tlnn16bHAAksc8++/T6VlRdPHwl3HdZkhQg+XnfZUm52U6gRycIoFcnhzKfgzqIgA2vw8JrtyaJ+y5L1je8vjVpmHVjPfoSk1ndSDA2bSksvDZ5AYz5QlLupG07gR7fguiuJk+ezO23395hnWHDhvHqq6/mPuZPf/pTpk6duqOhWbVkk0SZk4PtRJwgzIpSvqyUle2TMOvmnCDaOG/WE5w364mqHvOKK67g0EMP5cQTT2TixIl85zvf2Wb7gw8+yKhRoxgxYgSf+cxn2Lhx45ZtV199NSNGjGD06NGsXLkSgLvuuosxY8YwatQoPv7xj/PHP/6xqvFaFWT6HO7Z8yzO+8C9yeWlbJ+EWTfnBFGwpqYmfvnLX7JkyRLuvffe7caL2rBhA5MnT2bu3LksXbqUTZs2ce21127Z3q9fP5YuXcrUqVO55JJLADjxxBN58skneeqpp5gwYQJXX311TX8ny0HitmVvcM+eZ/HFteey8MV1nPfSOO7Z8yxo6OfLTFY1RfxRW+ZO6lT5BC98Ye0263MvPG6Hjvub3/yG8ePH09DQQENDA2eeeeY221esWEFjYyOHHHIIAJMmTWLmzJlbksHEiRO3/PzKV74CJM93nHfeebzyyiu8/fbbfr6hm7q97z8kLYW165ICiZvefyFnnHJ8fQMzy8kJopvL3qJaXr7ooov46le/yrhx41iwYAHTp0+vU3TWkfIfF9X6Y8Msq6g/arN8iSk198LjmHvhcYxpHMCYxgFb1nfUCSecwF133cWGDRtYv349d9999zbbDz30UF588cUt/Qs///nPOfnkk7fGNXfulp/HHZfE8/rrrzN48GAAbrrpph2O0cysErcgCnbssccybtw4jjzySPbbbz9GjBhBv379tmxvaGjgxhtv5Nxzz2XTpk0ce+yxfP7zn9+yfd26dRx55JHsvvvu3HrrrQBMnz6dc889l/79+/PRj36UF154oea/l+XnloMVoRYtVEWBd1NIGgtcA/QBro+Iq9psPwC4Cdg7rXNpRNyTbrsM+CywGfhyRNzf0XuVSqVo2wH87LPPcthhh1Xpt3nv1q9fz1577cVbb73FSSedxOzZszn66KNrGkN3ORdmVl07miAkLYqIUqVthbUgJPUBZgKnAi1Ak6R5EbE8U+3rwC8i4lpJhwP3AMPS5QnAEcBfAg9IOiQiNhcVb5GmTJnC8uXL2bBhA5MmTap5cjCznqvIFmqRl5hGAysjYhWApDnAeCCbIAJ4f7rcD/jvdHk8MCciNgIvSFqZHq+Ye7kKdsstt9Q7BDOzLiuyk3ow8HJmvSUty5oOfFpSC0nr4aIu7GtmZgWq911ME4GfRsQQ4Azg55JyxyRpiqRmSc2tra2FBWlm1hsVmSBWA0Mz60PSsqzPAr8AiIgngAZgYM59iYjZEVGKiNKgQYOqGLqZmRWZIJqA4ZIaJe1G0uk8r02dPwAfA5B0GEmCaE3rTZC0u6RGYDjw2wJjNTOzNgpLEBGxCZgK3A88S3K30jJJMySNS6v9b+ACSUuAW4HJkVhG0rJYDtwHfGlnvYNpr732qurxpk+fvt1gf2ZmRSj0Qbn0mYZ72pR9I7O8HDihnX2/CXyzyPgqvKnnDzYzS9W7k7r7KHj+4Ihg2rRpfOhDH2LEiBFbhtAA+Pa3v82IESMYOXIkl156KQDXXXcdxx57LCNHjuScc87hrbfeqkocZmZ5OUFATeYPvuOOO1i8eDFLlizhgQceYNq0abzyyivce++9/Md//AcLFy5kyZIl/NM//RMAn/zkJ2lqamLJkiUcdthh/OQnP9nhGMzMusJjMUFN5g9+7LHHmDhxIn369GG//fbj5JNPpqmpiUceeYTzzz+fPffcE4ABAwYA8Mwzz/D1r3+d1157jfXr13P66afvcAxmZl3hFkRZN5s/ePLkyfzwhz9k6dKlXH755WzYsKEucZhZ7+UEUVbw/MEf/vCHmTt3Lps3b6a1tZVHH32U0aNHc+qpp3LjjTdu6WNYuzYZ2/3NN99k//3355133uHmm2+uSgxmZl3hS0ywbZ9D+bJSeR2q0pI4++yzeeKJJxg5ciSSuPrqq/nABz7A2LFjWbx4MaVSid12240zzjiDb33rW1xxxRWMGTOGQYMGMWbMGN58880q/KJmZvkVOtx3Le3wcN8PX5l0SJeTQTlpNPSDUy7rfP9uzsN9m1kldRnue6dzymXbPvdQ7pPwcxBm1ku5DyKrbTJwcjCzXqzHJ4iecgltR/gcmNl70aMTRENDA2vWrOnVX5ARwZo1a2hoaKh3KGa2k+nRfRBDhgyhpaWF3j5XRENDA0OGDKl3GGa2k+nRCWLXXXelsbGx3mGYme2UevQlJjMze++cIMzMrCInCDMzq8gJwszMKnKCMDOzigpNEJLGSlohaaWkSyts/56kxenr95Jey2zbnNk2r8g4zcxse4Xd5iqpDzATOBVoAZokzUvnoQYgIr6SqX8RMCpziD9HxFFFxWdmZh0rsgUxGlgZEasi4m1gDjC+g/oTgVsLjMfMrGdpO0pElUeNKDJBDAZezqy3pGXbkXQg0Ag8lClukNQs6UlJZ7Wz35S0TnNvf1razHqZh6/cdlKz8hQFD1/Z8X5d0F06qScAt0fE5kzZgekY5Z8Cvi/p4LY7RcTsiChFRGnQoEG1itXMrL4ikvlrFl67NUmUJznb8HrVWhKd9kFIujgirumsrILVwNDM+pC0rJIJwJeyBRGxOv25StICkv6J5zuL18ysxyvPVwNJUijPflmeEbNKUxXkaUFMqlA2Ocd+TcBwSY2SdiNJAtvdjSTpg0B/4IlMWX9Ju6fLA4ETgOVt9zUz67WySaKsypOctduCkDSR5PJOY5vbTPsCazs7cERskjQVuB/oA9wQEcskzQCaI6J8zAnAnNh2TO7DgFmS3iVJYldl734yM+v1ypeVsu67rKpJoqNLTI8DrwADgf+bKX8TeDrPwSPiHuCeNmXfaLM+vcJ+jwMj8ryHmVmvk+1zKF9WKq9D1ZJEuwkiIl4CXgKOS+8yGh4RD0jaA9iDJFGYmVmtSdDQb9s+h/LlpoZ+NWlBpHHoAmAKMAA4mKSz+cfAx6oSgZmZdd0p6d1L5WRQThJV7IPI00n9JZJO4jcAIuI5YN+qRWBmZu9N22RQxeQA+RLExvRJ6PT9tQvQeyd5NjPrJfIkiEck/TOwh6RTgduAu4oNy8zM6i1PgrgUaAWWAheS3JX09SKDMjOz+uu0kzoi3gWuA66TNAAY0uaZBTMz64E6bUFIWiDp/WlyWESSKL5XfGhmZlZPeS4x9YuIN4BPAj+LiDH4Flczsx4vT4LYRdL+wN8Bdxccj5mZdRN5EsQMkvGUVkZEk6SDgOeKDauGCp5ww8xsZ5Wnk/o2kltby+urgHOKDKpmHr4yGTu9/PRheXyThn7JU4pmZr1Yd5kwqPZqNOGGmdnOqtMWRI9Vowk3zMx2Vnluc23MU7ZTqsGEG2ZmO6s8l5h+WaHs9moHUhftTbjhy0tmZh3OKPdB4Aign6RPZja9H2goOrDC1WjCDTOznVVHfRCHAn8D7A2cmSl/E7igyKBqokYTbpiZ7azU2bBKko6LiCfe08GlscA1JHNSXx8RV7XZ/j3glHR1T2DfiNg73TaJrYMC/mtE3NTRe5VKpWhubu56kNkJNyqtm5n1YJIWRUSp0rY8fRBnp2Mx7SrpQUmtkj6d4037ADOBTwCHAxMlHZ6tExFfiYijIuIo4N+BO9J9BwCXA2OA0cDlkvrniLXrCp5ww8xsZ5UnQZyWjsX0N8CLwP8CpuXYbzTJ09er0gmH5gDjO6g/Ebg1XT4dmB8RayNiHTAfGJvjPc3MrEryJIhd059/DdwWEa/nPPZg4OXMektath1JBwKNwENd2VfSFEnNkppbW1tzhmVmZnnkSRB3SfodcAzwoKRBwIYqxzEBuD0iNndlp4iYHRGliCgNGjSoyiGZmfVunSaIiLgUOB4oRcQ7wFt0fKmobDUwNLM+JC2rZAJbLy91dV8zMytAniep9wS+CKQPCPCXQMUe7zaagOGSGiXtRpIE5lU4/geB/kD2Tqn7gdMk9U87p09Ly8zMrEbyXGK6EXibpBUByV/y/9rZThGxCZhK8sX+LPCLiFgmaYakcZmqE4A52WlMI2ItcAVJkmkCZqRlZmZWI3meg2iOiJKkpyJiVFq2JCJG1iTCnN7zcxBmZr3Yjj4H8bakPYBID3YwsLGK8ZmZWTeUZ7jv6cB9wFBJNwMnAOcXGZSZmdVfnhnl/lPSIuCvAAEXR8SrhUdmZmZ1lecupgcjYk1E/Doi7o6IVyU9WIvgzMysfjoa7ruBZAC9gemtpuVBit5PO09Em5lZz9HRJaYLgUtInntYxNYE8Qbww4LjMjOzOms3QUTENcA1ki6KiH+vYUxmZtYN5Blqw8nBzKwXyvMchJmZ9UJOEGZmVlGe21wl6dOSvpGuHyBpdPGhmZlZPeVpQfwIOI5kxjeAN0mmEjUzsx4sz1AbYyLiaElPAUTEunT4bjMz68HytCDekdSHrYP1DQLeLTQqMzOruzwJ4gfAr4B9JX0TeAz4VqFRmZlZ3eUZrO/mdLC+j5E8TX1WRDxbeGRmZlZXefogAJ4jGWJjF0juZIqIPxQWlZmZ1V2e21wvAv4IzAfuBn6d/uyUpLGSVkhaKenSdur8naTlkpZJuiVTvlnS4vS13VzWZmZWrDwtiIuBQyNiTVcOnHZszwROBVqAJknzImJ5ps5w4DLghPTuqH0zh/hzRBzVlfc0M7PqydNJ/TLw+ns49mhgZUSsioi3gTnA+DZ1LgBmRsQ6gIj403t4HzMzK0BH80F8NV1cBSyQ9Gsyc1FHxHc7OfZgkuRS1gKMaVPnkPS9fgP0AaZHxH3ptgZJzcAm4KqIuLNCjFOAKQAHHHBAJ+GYmVlXdHSJqW/68w/pa7f0BekzEVV6/+HAR4AhwKOSRkTEa8CBEbFa0kHAQ5KWRsTz2Z0jYjYwG6BUKlUrJjMzo+P5IP4FQNK5EXFbdpukc3McezUwNLM+JC3LagEWRsQ7wAuSfk+SMJoiYnUaxypJC4BRwPOYmVlN5OmDuCxnWVtNwHBJjenQHBOAtncj3UnSekDSQJJLTqsk9Ze0e6b8BGA5ZmZWMx31QXwCOAMYLOkHmU3vJ+kX6FBEbJI0FbifpH/hhohYJmkG0BwR89Jtp0laDmwGpkXEGknHA7MkvUuSxK7K3v1kZmbFU0TlS/eSRgJHATOAb2Q2vQk8XL7zqLsolUrR3Nxc7zDMzHYqkhZFRKnSto76IJYASyTdkvYRmJlZL5JnTmonBzOzXshTjpqZWUVOEGZmVlGnYzFJOgSYBhyYrR8RHy0wLjMzq7M8g/XdBvwYuI7kVlQzM+sF8iSITRFxbeGRmJlZt5KnD+IuSV+UtL+kAeVX4ZGZmVld5WlBTEp/TsuUBXBQ9cMxM7PuIs+c1I21CMTMzLqXPHcx7Qp8ATgpLVoAzPIDdGZmPVueS0zXArsCP0rX/yEt+1xRQZmZWf3lSRDHRsTIzPpDkpYUFZCZmXUPee5i2izp4PJKOsObn4cwM+vh8rQgpgEPS1oFiOSJ6vMLjcrMzOouz11MD0oaDhyaFq2IiI3FhmVmZvWWpwVBmhCeLjgWMzPrRjyaq5mZVVRogpA0VtIKSSslXdpOnb+TtFzSMkm3ZMonSXoufU2qtK+ZmRUnz4NydwA/Ae6NiHfzHlhSH2AmcCrQAjRJmhcRyzN1hgOXASdExDpJ+6blA4DLgRLJsB6L0n271TzYZmY9WZ4WxI+ATwHPSbpK0qGd7ZAaDayMiFUR8TYwBxjfps4FwMzyF39E/CktPx2YHxFr023zgbE539fMzKogz5zUD0TE3wNHAy8CD0h6XNL56TAc7RkMvJxZb0nLsg4BDpH0G0lPShrbhX2RNEVSs6Tm1tbWzn4VMzPrglx9EJL2ASaTDK/xFHANScKYv4PvvwswHPgIMBG4TtLeeXeOiNkRUYqI0qBBg3YwFDMzy8rTB/Erkmcgfg6cGRGvpJvmSmruYNfVwNDM+pC0LKsFWJgO/PeCpN+TJIzVJEkju++CzmI1M7PqydOC+EFEHB4RV2aSAwARUepgvyZguKRGSbsBE4B5bercSZoIJA0kueS0CrgfOE1Sf0n9gdPSMjMzq5E8CeLw7GWf9Ev7i53tFBGbgKkkX+zPAr+IiGWSZkgal1a7H1gjaTnwMDAtItZExFrgCpIk0wTMSMvMzKxGFBEdV5AWR8RRbcqeiohRhUbWRaVSKZqbO7riZWZmbUla1N7VoDwtiD6SlDlYH2C3agVnZmbdU56xmO4j6ZCela5fmJaZmVkPlidB/B+SpPCFdH0+cH1hEZmZWbeQZ7jvd0mmGL22+HDMzKy7yPMcxHDgSuBwoKFcHhEHFRiXmZnVWZ5O6htJWg+bgFOAnwH/r8igzMys/vIkiD0i4kGSW2JfiojpwF8XG5aZmdVbnk7qjZL+gmQ016kkw2DsVWxYZmZWb3laEBcDewJfBo4BPg14Ah8zsx6uwxZE+lDceRHxj8B64PyaRGVmZnXXYQsiIjYDJ9YoFjMz60by9EE8JWkecBvwP+XCiLijsKjMzKzu8iSIBmAN8NFMWQBOEGZmPVieJ6nd72Bm1gvleZL6RpIWwzYi4jOFRGRmZt1CnktMd2eWG4Czgf8uJhwzM+su8lxi+mV2XdKtwGOFRWRmZt1Cngfl2hoO7FvtQMzMrHvpNEFIelPSG+UXcBfJHBGdkjRW0gpJKyVdWmH7ZEmtkhanr89ltm3OlM/ryi9lZmY7Ls8lpr7v5cDpU9gzgVOBFqBJ0ryIWN6m6tyImFrhEH9uOxe2mZnVTp4WxNmS+mXW95Z0Vo5jjwZWRsSqiHgbmAOMf++hmplZLeXpg7g8Il4vr0TEa8DlOfYbDLycWW9Jy9o6R9LTkm6XNDRT3iCpWdKT7SUkSVPSOs2tra05QjIzs7zyJIhKdfLcHpvHXcCwiDiSZK7rmzLbDoyIEvAp4PuSDm67c0TMjohSRJQGDRpUpZDMzAzyJYhmSd+VdHD6+i6wKMd+q4Fsi2BIWrZFRKyJiI3p6vUkw4mXt61Of64CFgCjcrynmZlVSZ4EcRHwNjCXpB9hA/ClHPs1AcMlNUraDZgAbHM3kqT9M6vjgGfT8v6Sdk+XBwInAG07t83MrEB57mL6H2C7W1Rz7LcpnYHufqAPcENELJM0A2iOiHnAlyWNI5nvei0wOd39MGCWpHdJkthVFe5+MjOzAiliu2GWtq0gzQfOTTunkdQfmBMRp9cgvtxKpVI0NzfXOwwzs52KpEVpf+928lxiGlhODgARsQ4/SW1m1uPlSRDvSjqgvCLpQCqM7mpmZj1LnttVvwY8JukRQMCHgSmFRmVmZnWXp5P6PklHA3+VFl0SEa8WG5aZmdVb3gfeNgN/IpkP4nBJRMSjxYVlZmb1lmdGuc8BF5M86LaYpCXxBNvOUW1mZj1Mnk7qi4FjgZci4hSSJ5pf63gXMzPb2eVJEBsiYgOApN0j4nfAocWGZWZm9ZanD6JF0t7AncB8SeuAl4oNy8zM6i3PXUxnp4vTJT0M9APuKzQqMzOruy4N2x0RjxQViJmZdS95+iDMzKwXcoIwM7OKnCDMzKwiJwgzM6vICcLMzCpygjAzs4oKTRCSxkpaIWmlpO2mLZU0WVKrpMXp63OZbZMkPZe+JhUZp5mZba9Lz0F0haQ+wEzgVKAFaJI0r8Lc0nMjYmqbfQcAlwMlksmJFqX7risqXjMz21aRLYjRwMqIWBURbwNzgPE59z0dmB8Ra9OkMB8YW1CcZmZWQZEJYjDwcma9JS1r6xxJT0u6XdLQruwraYqkZknNra2t1YrbzMyofyf1XcCwiDiSpJVwU1d2jojZEVGKiNKgQYMKCdDMrLcqMkGsBoZm1oekZVtExJqI2JiuXg8ck3dfMzMrVpEJogkYLqlR0m7ABGBetoKk/TOr44Bn0+X7gdMk9ZfUHzgtLTMzsxop7C6miNgkaSrJF3sf4IaIWCZpBtAcEfOAL0saB2wC1gKT033XSrqCJMkAzIiItUXFamZm21NE1DuGqiiVStHc3FzvMMzMdiqSFkVEqdK2endSm5lZN+UEYWZmFTlBmJlZRU4QZmZWkROEmZlV5ARhZmYVOUGYmVlFThBmZlaRE4SZmVXkBGFmZhU5QZiZWUVOEGZmVpEThJmZVeQEYWZmFTlBmJlZRU4QZmZWkROEmZlV5ARhZmYVFZogJI2VtELSSkmXdlDvHEkhqZSuD5P0Z0mL09ePi4zzvFlPcN6sJ4p8CzOznc4uRR1YUh9gJnAq0AI0SZoXEcvb1OsLXAwsbHOI5yPiqKLiMzOzjhWWIIDRwMqIWAUgaQ4wHljept4VwLeBaQXGUlG51bDwhbXbrM+98Lhah2Jm1u0UeYlpMPByZr0lLdtC0tHA0Ij4dYX9GyU9JekRSR+u9AaSpkhqltTc2tpatcDNzKzYFkSHJP0F8F1gcoXNrwAHRMQaSccAd0o6IiLeyFaKiNnAbIBSqRRdjaHcUnDLwcxse0W2IFYDQzPrQ9Kysr7Ah4AFkl4E/gqYJ6kUERsjYg1ARCwCngcOKTBWMzNro8gWRBMwXFIjSWKYAHyqvDEiXgcGltclLQD+MSKaJQ0C1kbEZkkHAcOBVUUF6paDmdn2CksQEbFJ0lTgfqAPcENELJM0A2iOiHkd7H4SMEPSO8C7wOcjYm1RsZqZ2fYU0eVL991SqVSK5ubmeodhZrZTkbQoIkqVtvlJajMzq8gJwszMKnKCMDOzipwgzMysoh7TSS2pFXhpBw4xEHi1SuFUk+PqGsfVNY6ra3piXAdGxKBKG3pMgthRkprb68mvJ8fVNY6raxxX1/S2uHyJyczMKnKCMDOzipwgtppd7wDa4bi6xnF1jePqml4Vl/sgzMysIrcgzMysIicIMzOrqMcnCEljJa2QtFLSpRW27y5pbrp9oaRhmW2XpeUrJJ1e47i+Kmm5pKclPSjpwMy2zZIWp6+ORsUtIq7Jkloz7/+5zLZJkp5LX5NqHNf3MjH9XtJrmW1Fnq8bJP1J0jPtbJekH6RxP53OoljeVuT56iyuv0/jWSrpcUkjM9teTMsXS6rqCJg54vqIpNcz/17fyGzr8DNQcFzTMjE9k36mBqTbijxfQyU9nH4XLJN0cYU6xX3GIqLHvkiGGX8eOAjYDVgCHN6mzheBH6fLE4C56fLhaf3dgcb0OH1qGNcpwJ7p8hfKcaXr6+t4viYDP6yw7wCSOTsGAP3T5f61iqtN/YtIhpcv9Hylxz4JOBp4pp3tZwD3AiKZFGth0ecrZ1zHl98P+EQ5rnT9RWBgnc7XR4C7d/QzUO242tQ9E3ioRudrf+DodLkv8PsK/ycL+4z19BbEaGBlRKyKiLeBOcD4NnXGAzely7cDH5OktHxOJLPbvQCsTI9Xk7gi4uGIeCtdfZJkRr6i5Tlf7TkdmB8RayNiHTAfGFunuCYCt1bpvTsUEY8CHc1VMh74WSSeBPaWtD/Fnq9O44qIx9P3hdp9vvKcr/bsyGez2nHV8vP1SkT8V7r8JvAsMLhNtcI+Yz09QQwGXs6st7D9yd1SJyI2Aa8D++Tct8i4sj5L8hdCWYOkZklPSjqrSjF1Ja5z0qbs7ZLK08p2i/OVXoprBB7KFBd1vvJoL/Yiz1dXtf18BfCfkhZJmlKHeI6TtETSvZKOSMu6xfmStCfJl+wvM8U1OV9KLn+PAha22VTYZ6zIKUetCiR9GigBJ2eKD4yI1UqmY31I0tKIeL5GId0F3BoRGyVdSNL6+miN3juPCcDtEbE5U1bP89WtSTqFJEGcmCk+MT1f+wLzJf0u/Qu7Fv6L5N9rvaQzgDtJphzuLs4EfhPbznBZ+PmStBdJUrokIt6o5rE70tNbEKuBoZn1IWlZxTqSdgH6AWty7ltkXEj6OPA1YFxEbCyXR8Tq9OcqYAHJXxU1iSsi1mRiuR44Ju++RcaVMYE2zf8Cz1ce7cVe5PnKRdKRJP+G4yNiTbk8c77+BPyK6l1a7VREvBER69Ple4BdJQ2kG5yvVEefr0LOl6RdSZLDzRFxR4UqxX3GiuhY6S4vkhbSKpJLDuWOrSPa1PkS23ZS/yJdPoJtO6lXUb1O6jxxjSLplBveprw/sHu6PBB4jip11uWMa//M8tnAk7G1Q+yFNL7+6fKAWsWV1vsgSYehanG+Mu8xjPY7Xf+abTsQf1v0+coZ1wEk/WrHtyl/H9A3s/w4MLaGcX2g/O9H8kX7h/Tc5foMFBVXur0fST/F+2p1vtLf/WfA9zuoU9hnrGont7u+SHr4f0/yZfu1tGwGyV/lAA3Abel/lt8CB2X2/Vq63wrgEzWO6wHgj8Di9DUvLT8eWJr+B1kKfLbGcV0JLEvf/2Hgg5l9P5Oex5XA+bWMK12fDlzVZr+iz9etwCvAOyTXeD8LfB74fLpdwJ39ayAAAAPUSURBVMw07qVAqUbnq7O4rgfWZT5fzWn5Qem5WpL+O3+txnFNzXy+niSTwCp9BmoVV1pnMsmNK9n9ij5fJ5L0cTyd+bc6o1afMQ+1YWZmFfX0PggzM3uPnCDMzKwiJwgzM6vICcLMzCpygjAzs4qcIMyqRNI/t1l/vOD3WyCplC7fI2nv93icsyQdXt3orCdwgrAeIx32uJ6f6W0SREQcX6s3jogzIuK1zmtWdBbJ6MVm23CCsJ2apGHpHAE/A54BhqZj9zelAwr+S6bu15TMFfGYpFsl/WNanv1LfKCkF9PlPpL+LXOsC9Py/SU9mpkb4MOSrgL2SMtuTuutT39+JH2P2yX9TtLN6YjBSDojLVuUjul/d4XfsY+k76Tv9bSkiyrUeTEdkgJJn5b02zSWWZL6lOOR9M10ILwnJe0n6XhgHPBvaf2Dq/VvYzs/JwjrCYYDP4qII4BD0/XRwFHAMZJOknQMyVAqR5E8iXpsjuN+Fng9Io5N618gqRH4FHB/RBwFjAQWR8SlwJ8j4qiI+PsKxxoFXELyl/pBwAmSGoBZJE/pHwMMaieOKSTDQBwVEUcCN7cXsKTDgPOAE9L4NgPleN5HMjTKSOBR4IKIeByYB0xLY/cghraFR3O1nuClSMbBBzgtfT2Vru9FkjD6Ar+KdI4N5ZtZ7jTgSEl/m673S4/VBNyQDqJ2Z0QsznGs30ZES/rei0m+8NcDqyKZbwSS4R4qDRf9cZLxwjYBxLYjibb1MZIBFJvSRsoewJ/SbW8D5RbKIuDUHHFbL+YEYT3B/2SWBVwZEbOyFSRd0sH+m9jamm5oc6yLIuL+tjtIOolkkLSfSvpuRPyskxg3ZpY3U9z/PQE3RcRlFba9E1vH1ikyBushfInJepr7gc+k4+cjaXA6Tv+jwFmS9pDUl2Rc/7IX2Tps+d+2OdYX0pYCkg6R9L50UqI/RsR1JIPelecAfqdcN6cVwEHaOg/6ee3Umw9cmA5Hj9K5kNvxIPC36e+MpAHKzGfejjdJWlhm23CCsB4lIv4TuAV4QtJSkmlk+0YybeNcklE37yW5TFT2HZJE8BTJkOBl1wPLgf9SMpn9LJK/uj8CLEnrnwdck9afDTxd7qTOEeufSeZEv0/SIpIv6tcrVL2eZNjrpyUtIekDae+Yy4Gvk8xw9jRJctm/k1DmANMkPeVOasvyaK7WK0maDqyPiO/UOY69Ipk9rTxk83MR8b16xmRW5haEWX1dkHZaLyPpBJ/VSX2zmnELwszMKnILwszMKnKCMDOzipwgzMysIicIMzOryAnCzMwq+v954iNPbtXUdgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "markers = {'global': '+', 'local': 'x'}\n",
    "for key, val in test_scores.items():\n",
    "    x = np.arange(len(val))\n",
    "    plt.scatter(x, val, label=key, marker=markers[key])\n",
    "plt.legend()\n",
    "plt.xlabel('requesting client')\n",
    "plt.ylabel('accuracy on the test set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global': [0.45, 0.81428576, 0.81111115],\n",
       " 'local': [0.7125, 0.8214286, 0.74444443]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EaE23XWAxE2x"
   },
   "source": [
    "# Questions\n",
    "\n",
    "1. Does the net.init also initialize the output layer based on batch size?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Jax federated learning",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "JaxEnv",
   "language": "python",
   "name": "jaxenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
